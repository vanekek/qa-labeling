# qa_labeling

## Описание проекта

### Постановка задачи

Я взял задачу из
[соревнования на kaggle](https://www.kaggle.com/c/google-quest-challenge/overview).
Задача состоит в мультилейбл классификации различных пар текстов вопрос-ответ по
различным категориям, таким как релевантность ответа, полнота ответа,
субъективен ли вопрос, грамотно ли написан вопрос и ответ и т.д. Данные
собирались вручную командой гугла со stackexchange. Нужно это с целью сбора
информации и последующей аналитики для построения “умных” q&a систем. Ровно
такая задача может быть и например в банке в отделе чат-бота.

### Формат входных и выходных данных

Данные состоят из двух текстов вопрос-ответ с доп. фичами для каждого (имя
пользователя, ссылка, заголовок, само тело вопроса, имя предметной ветки на
stackexchange и др.), а также есть суммарно 30 таргетов, которые должна
предсказывать модель, каждый таргет это бинарная метка 0 или 1, но поскольку
датасет собирался вручную и разметка проводилась краудсорс командой, то их
оценки агрегировались и необходимо предсказывать вероятность в отрезке [0, 1].

### Метрики

Главная метрика - средний по колонкам
[коэффициент корреляции Спирмена](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient).
Эта метрика тут релевантна, поскольку таргеты у нас из-за усреднения по
разметчикам это непрерывные случайные величины на отрезке [0,1], поэтому
дефолтные accuracy, precision, f1 посчитать мы просто так не сможем, для нас
важна общая согласованность предсказываемых значений модели с истинными
значениями без учёта конкретного порога, что по рангам значений в общей выборке
и будет показывать коэф. кор. Спирмена.

### Валидация

Есть отложенные train и test выборки, валидационную выборку я засемплил из
тренировочной (15% от train), воспроизводимость буду контролировать с помощью
установки конкретного random_state.

### Данные

Данные предоставлены гуглом, собирала их внутренняя команда краудсорса больше
чем два года, размечали их разметчики. Проблем в целом быть не должно, поскольку
это данные с соревнования с kaggle, в целом их особенность в том, что пары
вопрос-ответ подобраны из совершенно разных доменов (программирование или
кулинария) и в том, что как вопрос, так и ответ могут быть очень субъективны,
разметчикам не давались конкретные инструкции, поэтому модели может быть сложно
выявить закономерности.

### Моделирование

Взял бейзлайн решение из
[ноутбука](https://www.kaggle.com/code/phoenix9032/pytorch-bert-plain/notebook),
решение представляет собой файнтюнинг BERT модели (bert-base-uncased) с
классификационной головой. У меня нет возможности обучать на гпу (на рабочий
кластер идти не хочется), делаю все на цпу. Обучается модель с помощью AdamW и
кастомных lr для весов, в качестве функции потерь беру BCE на логитах.

### Внедрение

Я предполагаю, что я делаю модель для прода в компании, есть конвертация в onnx
формат и простой инференс на торче.

## Setup

В качестве менеджера зависимостей полностью использован uv (пытался сделать
conda + uv, но не понял как заставить uv работать именно с env'ом конды, все
равно создается .venv, а просто с `uv pip install` не интересно).

У вас должен быть установлен uv: `pip install uv`

После клонирования репозитория необходимо сделать `uv venv` в корне проекта,
затем активировать venv: `source .venv/bin/activate`, затем установить
зависимости с помощью `uv pip install  -e .`. Будет создано виртуальное
окружение готовое к работе с пакетом.

Данные версионируются с помощью dvc, я сделал отдельный репозиторий куда
загрузил данные (они небольшие, 10-20 Мб), я долго пытался подружить dvc и
google drive, но не смог, поскольку не хочется усложнять себе жизнь с передачей
кредов, выбрал такой вариант, знаю что гит не предназначен для данных.
Необходимо выполнить в окружении `python scripts/download_data.py`, после чего
выполнить `dvc_pull`. Далее данные будут сами подтягиваться в скриптах с помощью
`dvc.api`.

## Train

Для обучения модели используется pytorch-lightning, логирование происходит на
локальный mlflow, вся конфигурация эксперимента лежит в папке `conf`.

Сначала необходимо в отдельном терминале поднять локальный инстанс mlflow
`mlflow server --host 127.0.0.1 --port 8080`.

Далее обучение запускается с помощью команды `python -m qa_labeling.train`. Логи
можете смотреть в mlflow по указанному адресу, также в папке `models` будет
лежать чекпоинт с моделью.

## Production preparation

Есть экспорт модели в onnx, запускается с помощью команды
`python -m qa_labeling.export_to_onnx`, также необходимо указать путь
`onnx_path` к чекпоинту модели в `conf/inference/inference.yaml`.

## Infer

Имеется скрипт infer.py в котором происходит инференс на тестовых данных
локально, запуск через `python -m qa_labeling.infer`, также нужно указать путь к
чекпоинту в `conf/inference/inference.yaml`.
